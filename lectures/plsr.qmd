---
title: Partial Least Squares Regression
author: Aaron Meyer (based on slides from Pam Kreeger)
---

::: {.notes}
- Explain unsupervised/supervised.
- Can you bootstrap a PLSR/PCR/PCA model?
:::

# Common Challenge: Cue/Signal/Response Relationships

![ ](./figs/plsr/CSR-figure.pdf)

::: {.notes}
- Number of components enough to make PCA/lasso useful
- But how do we deal with components moving together?
- And what we care about is some outcome
:::

## Application

![ ](./figs/plsr/lee-1.webp)

## Application

![ ](./figs/plsr/lee-2.webp)

## Application

![ ](./figs/plsr/lee-4.webp)

# Many Methods for Relating a Signal to Response

Say we have some measurement from cells and how they respond:

$$ [1, 2, 1.5, 5, 6, 7] \sim [5, 10, 7, 24, 31, 35] $$

From the variation we can see that:

- Low signal is correlated with low response
- High signal is correlated with high response

If we can find a quantitative correlation between the input and output, we can predict new outcomes for measurements we haven't yet seen.

::: {.notes}
Draw out correlation and then include new point.
:::

# Challenges with Univariate Relationships

![Janes, et al. Science, 2005](./figs/plsr/Janes-fig.jpg)

- The relationship between JNK activation and apoptosis appears to be highly context-dependent
	- Univariate relationships are often insufficient
	- Cells respond to an environment with multiple factors present

# Notes about Methods Today

- Both methods are supervised learning methods, however have a number of distinct properties from others we will discuss.
- Learning about PLS is more difficult than it should be, partly because papers describing it span areas of chemistry, economics, medicine and statistics, with little agreement on terminology and notation.
- These methods will show one example of where the model and algorithm are quite distinct—there are multiple algorithms for calculating a PLSR model.

## Regularization

- Both PCR and PLSR are forms of regularization.
- Reduce the dimensionality of our regression problem to $N_{comp}$.
- Prioritize certain variance in the data.

# Principal Components Regression

## Principal Components Regression

One solution - use the concepts from PCA to reduce dimensionality.

First step: **Simply apply PCA!**

Dimensionality goes from $m$ to $N_{comp}$.

::: {.notes}
- What are the bounds on the number of components we can have?
- What might be a concern as a consequence of this? (How do we determine how many components to keep?)
:::

# Principal Components Regression (PCR)

1)  Decompose X matrix (scores T, loadings P, residuals E)
$$X = TP^T + E$$

2) Regress Y against the scores (Scores describe observations – by using them we link X and Y for each observation)

$$Y = TB + E$$

::: {.notes}
- What is the residual against here?
- How do we invert the direction of the regression?
:::

# Challenge

**How might we determine the number of components using our prediction?**

::: {.notes}
- Cross-validation with varying numbers of components
- Metrics like AIC or BIC
:::

# Potential Problem

- PCs for the X matrix do not necessarily capture X-variation that is important for Y
	- So later PCs are going to be more important to regression
- Example: the first components capture signaling that is related to another cell fate, while the signals that co-vary for this particular y are buried in later components

**How might we handle this differently?**

# PLSR

![ ](./figs/plsr/PC-5.pdf)

::: {.notes}
- Go over definition of covariance.
- cov(X, X) = var(X)
- var(Y) = cov(X, Y) + variance in Y independent of X
- A factorization occurs to find max covar with Y
- Forces direction of components
:::

# PLSR - NIPALs with Scores Exchanged

![ ](./figs/plsr/PC-6.pdf)

::: {.notes}
- $B$ is $T (P^T T)^{-1} Q^T$
- $P^T T$ is the approximation of $X$
- PLSR components are not orthogonal in data space, but are orthogonal in O-PLS.
:::

# PLSR - NIPALs with Scores Exchanged

![ ](./figs/plsr/PC-7.pdf)

# Components in PLSR and PCA Differ

![ ](./figs/plsr/PC-8.pdf)

## Determining the Number of Components

R2X provides the variance explained in X:

$$ R^2X = 1 - \frac{\lvert X_{\textrm{PLSR}} - X \rvert }{\lvert X \rvert} $$

R2Y shows the Y variance explained:

$$ R^2Y = 1 - \frac{\lvert Y_{\textrm{PLSR}} - Y \rvert }{\lvert Y \rvert} $$

If you are trying to predict something, you should look at the cross-validated R2Y (a.k.a. Q2Y).

## Application

![ ](./figs/plsr/lee-5.pdf)

## Application

![ ](./figs/plsr/lee-6.pdf)

## Application

![ ](./figs/plsr/lee-7.pdf)

# Practical Notes & Summary

## PCR

- sklearn does not implement PCR directly
- Can be applied by chaining [`sklearn.decomposition.PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and [`sklearn.linear_model.LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
- See: [`sklearn.pipeline.Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)

## PLSR

- [`sklearn.cross_decomposition.PLSRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)
	- Uses `M.fit(X, Y)` to train
	- Can use `M.predict(X)` to get new predictions
	- `PLSRegression(n_components=3)` to set number of components on setup
	- Or `M.n_components = 3` after setup

## Comparison

### PLSR

- Maximizes the covariance
- Takes into account both the dependent (Y) and independent (X) data

### PCR

- Uses PCA as initial decomp. step, then is just normal linear regression
- Maximizes the variance explained of the independent (X) data

## Performance

- PLSR is amazingly well at prediction
	- This is **incredibly** powerful
- Interpreting **why** PLSR predicts something can be very challenging

## Review Questions

1. What are three differences between PCA and PLSR in implementation and application?
2. What is the difference between PCR and PLSR? When does this difference matter more/less?
3. How might you need to prepare your data before using PLSR?
4. How can you determine the right number of components for a model?
5. What feature of biological data makes PLSR/PCR superior to direct regularization approaches (LASSO/ridge)?
6. What benefit does a two component model have over those with 3+ components?
7. Can you apply K-fold cross-validation to a PLSR model? If so, when do you scale your data?
8. Can you apply bootstrapping to a PLSR model? Describe what this would look like.
9. You use the same X data but want to predict a different Y variable. Do your X loadings change in your new model? What about for PCR?
