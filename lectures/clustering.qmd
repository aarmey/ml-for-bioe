---
title: Clustering
author: Aaron Meyer (based on slides from David Sontag)
---

# Clustering

- Unsupervised learning
- Requires data, but no labels
- Detect patterns e.g. in
	- Gene expression between patient samples
	- Images
	- Really any sets of measurements across samples
- Useful when don’t know what you’re looking for
- But: **can be gibberish**

## Clustering

- Basic idea: group together similar instances
- Example: 2D point patterns

![ ](./figs/clustering/clust1.png){fig-alt="Groups of points, circled to be placed within two clusters."}

::: {.notes}
Go over what is the data input and output (discrete).
:::

## Clustering is subjective!

::: {.notes}
Go over example of the Simpsons
- Lisa
- Bart
- Maggie
- Marge
- Krusty
- Sideshow Bob
:::

# Applications

- Clustering patients into groups based on molecular or etiological measurements
- Cells into groups based on molecular measurements
- Neuronal signals 

## 

![ ](./figs/clustering/BCclustersTitle.webp){fig-alt="Title slide or header image for Breast Cancer clusters."}

## 

![ ](./figs/clustering/BCclusters.webp){fig-alt="Heatmap or plot showing clustering of breast cancer patients based on molecular measurements."}

::: {.notes}
- We use clusters because they (hopefully) translate to meaningful differences.
- What if you cluster based on different measurements?
:::

# What does "similar" mean?

## What does "similar" mean?

- One option: Euclidean distance

$$ \textrm{dist}(\mathbf{x}, \mathbf{y}) = \lVert \mathbf{x} - \mathbf{y} \rVert $$

- Clustering results are **completely** dependent on the measure of similarity (or distance) between observations to be clustered

## What properties should a distance measure have?

### Symmetric

- $D(A, B) = D(B, A)$
- Otherwise we can say `A` looks like `B` but not vice versa

### Positivity and self-similarity

- $D(A,B) > 0$, and $D(A,B)=0$ iff $A=B$
- Otherwise there will be objects that we can't tell apart

### Triangle inequality

- $D(A, B) + D(B, C) \geq D(A,C)$
- Otherwise one can say "`A` like `B`, `B` like `C`, but `A` not like `C` at all"

# Clustering algorithms

::: columns

:::: column
- Partition algorithms (flat)
	- K-means
	- Gaussian mixtures

- Hierarchical algorithms
	- Bottom up - agglomerative
	- Top down - divisive
::::

:::: column
![ ](./figs/clustering/simpsons.webp){fig-alt="Characters from the Simpsons, clustered according to partitioning or agglomerative clustering."}
::::

:::

# K-means, an iterative clustering algorithm

::: columns

:::: column
- Initialize: Pick K random points as cluster centers
- Alternate until no assignments change:
	1. Assign data points to the closest cluster center
	2. Change the cluster center to the average of its assigned points
::::

:::: column
![ ](./figs/clustering/kmeans1.png){fig-alt="Illustration of K-means initialization: random cluster centers are picked."}
::::

:::

## K-means, an iterative clustering algorithm

::: columns

:::: column
- Initialize: Pick K random points as cluster centers
- Alternate until no assignments change:
	1. Assign data points to the closest cluster center
	2. Change the cluster center to the average of its assigned points
::::

:::: column
![ ](./figs/clustering/kmeans2.png){fig-alt="Illustration of K-means step: data points assigned to closest centers."}
::::

:::

## K-means clustering: Example

- Pick K random points as cluster centers (means)
	- Shown here for K=2

![ ](./figs/clustering/k1.png){fig-alt="Step 1: K random points are chosen as initial cluster centers."}

## K-means clustering: Example

### Iterative Step 1

Assign data points to the closest cluster center

![ ](./figs/clustering/k2.png){fig-alt="Step 2: Data points are assigned to the nearest cluster center."}

::: {.notes}
Critical step is right here. How do we pick what is close? Which is closer?
:::

## K-means clustering: Example

### Iterative Step 2

Change the cluster center to the average of the assigned points

![ ](./figs/clustering/k3.png){fig-alt="Step 3: Cluster centers are updated to the average of assigned points."}

::: {.notes}
- Can be slow for very large numbers of data points.
- Can use stochastic sampling in some cases.
:::

## K-means clustering: Example

Repeat until convergence

![ ](./figs/clustering/k4.png){fig-alt="Step 4: Repeat assignment of points to new centers."}

## K-means clustering: Example

![ ](./figs/clustering/k5.png){fig-alt="Step 5: Centers update again."}

## K-means clustering: Example

![ ](./figs/clustering/k6.png){fig-alt="Final step: Convergence, assignments do not change."}


## An example

![https://commons.wikimedia.org/wiki/User:Chire](./figs/clustering/K-means_convergence.gif){fig-alt="Animation showing the iterative process of K-means clustering, where centroids move and partition the data until convergence."}

## Properties of K-means algorithm

- Guaranteed to converge in a finite number of iterations
- Running time per iteration:
	1. Assign data points to the closest cluster center:
		- **O(KN) time**
	2. Change the cluster center to the average of its assigned points:
		- **O(N) time**

## K-Means convergence

**Objective:** $\min_{\mu}\min_{C}\sum_{i=1}^k \sum_{x\in C_i} \lVert x - \mu_i \rVert^2$

1. Fix $\mu$, optimize $C$: $$\min_C \sum_{i=1}^k \sum_{x\in C_i} \lVert x - \mu_i \rVert^2 = \min_C \sum_{i}^n \lVert x_i - \mu_{x_i} \rVert^2$$
2. Fix $C$, optimize $\mu$: $$\min_{\mu}\sum_{i=1}^k \sum_{x\in C_i} \lVert x - \mu_i \rVert^2$$
	- Take partial derivative of $\mu_i$ and set to zero, we have $$\mu_i = \frac{1}{\lVert C_i \rVert} \sum_{x \in C_i} x$$

Kmeans takes an alternating optimization approach. Each step is guaranteed to decrease the objective—thus guaranteed to converge.

## K-means is (somewhat) sensitive to initialization

::: columns

:::: column
- Various heuristic schemes exist for preventing problematic results.
- None of them are perfect.
::::

:::: column
![ ](./figs/clustering/init.png){fig-alt="Comparison of two different K-means results on the same data, showing how different initializations can lead to different clusterings."}
::::

:::

::: {.notes}
- Also go through cluster identity invariance.
:::

## K-Means Getting Stuck

Local optima dependent on how the problem was specified:

![ ](./figs/clustering/localopt.png){fig-alt="Example of K-means getting stuck in a local optimum, where the resulting clusters are suboptimal compared to the global optimum."}

## K-means not able to properly cluster

![ ](./figs/clustering/circle.png){fig-alt="K-means failing to cluster concentric circles, as it separates them linearly."}

::: {.notes}
- Go over number of clusters.
- Show example data.
- What would within-cluster distance look like in the ideal case?
:::

## Changing the features (distance function) can help

![ ](./figs/clustering/circletrans.png){fig-alt="Successful clustering of the concentric circles after transforming the features (e.g., using polar coordinates or kernel method)."}

::: {.notes}
- This is the kernel method.
- Will use again with SVMs!
:::

# Agglomerative Clustering

## Agglomerative Clustering {.smaller}

::: columns

:::: {.column width=60%}
- Agglomerative clustering:
	- First merge very similar instances
	- Incrementally build larger clusters out of smaller clusters
- Algorithm:
	- Maintain a set of clusters
	- Initially, each instance in its own cluster
	- Repeat:
		- Pick the two closest clusters
		- Merge them into a new cluster
		- Stop when there’s only one cluster left
- Produces not one clustering, but a family of clusterings represented by a dendrogram
::::

:::: {.column width=40%}
![ ](./figs/clustering/agg1.png){fig-alt="Schematic of agglomerative clustering: initially each point is a cluster, then closest pairs are merged."}
::::

:::

## Agglomerative Clustering

How should we define “closest” for clusters with multiple elements?

![ ](./figs/clustering/agg2.png){fig-alt="Illustration of measuring distance between two clusters with multiple points."}

## Agglomerative Clustering

- How should we define “closest” for clusters with multiple elements?
- Many options:
	- [Closest pair]{style="color:darkgreen;"} (single-link clustering)
	- [Farthest pair]{style="color:darkred;"} (complete-link clustering)
	- Average of all pairs
- Different choices create different clustering behaviors

![ ](./figs/clustering/agg3.png){fig-alt="Comparison of single-link, complete-link, and average-link distance measures between clusters."}

## Agglomerative Clustering

- How should we define “closest” for clusters with multiple elements?
- Many options:
	- Closest pair (left)
	- Farthest pair (right)
	- Average of all pairs
- Different choices create different clustering behaviors

![ ](./figs/clustering/agglo.png){fig-alt="Visual comparison of clustering results using single-link (closest pair) versus complete-link (farthest pair) criteria."}

# Clustering Behavior

![Mouse tumor data from Hastie et al.](./figs/clustering/clustbeh.png){fig-alt="Dendrogram showing the hierarchical clustering of mouse tumor data."}

## Agglomerative Clustering Questions

- Will agglomerative clustering converge?
	- To a global optimum?
- Will it always find the true patterns in the data?
- Do people ever use it?
- How many clusters to pick?

## Reconsidering “hard assignments”?

- Clusters may overlap
- Some clusters may be “wider” than others
- Distances can be deceiving!

![ ](./figs/clustering/olap.svg){fig-alt="Diagram showing overlapping probability distributions, illustrating that clusters can overlap and hard assignments might be misleading."}

## Review

![ ](./figs/clustering/review-title.webp){fig-alt="Review section title slide."}

## 

![ ](./figs/clustering/review-fig1.webp){fig-alt="Review figure 1."}

## 

![ ](./figs/clustering/review-fig2.webp){fig-alt="Review figure 2."}

## 

![ ](./figs/clustering/review-fig3.webp){fig-alt="Review figure 3."}

## 

![ ](./figs/clustering/review-fig4.webp){fig-alt="Review figure 4."}

# Implementation

## K-means

[`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)

- `n_clusters`: Number of clusters to form
- `init`: Initialization method
- `n_init`: Number of initializations
- `max_iter`: Maximum steps algorithm can take

## Agglomerative

[`sklearn.cluster.AgglomerativeClustering`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)

- `n_clusters`: Number of clusters to return
- `metric`: Distance metric to use
- `linkage`: Linkage metric to use

## Fisher's iris dataset

![https://en.wikipedia.org/wiki/Iris_flower_data_set](./figs/clustering/iris1.webp){fig-alt="Photograph of Iris flowers (setosa, versicolor, virginica) used in the dataset."}

## K-means implementation

```{.python}
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn import datasets

iris = datasets.load_iris()
X, y = iris.data, iris.target

est = KMeans(n_clusters=3)
est.fit(X)
labels = est.labels_
# ...
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels)
```

## 

![ ](./figs/clustering/iris2.svg){fig-alt="PCA plot of the Iris dataset showing the results of K-means clustering. The points are colored by their assigned cluster (teal, purple, yellow) plotted against the first two principal components."}

## 

![ ](./figs/clustering/iris3.svg){fig-alt="PCA plot of the Iris dataset colored by the true species labels (setosa, versicolor, virginica). This allows comparison with the K-means clustering results."}

# Review

## Further Reading

- [sklearn: Clustering](https://scikit-learn.org/stable/modules/clustering.html)
- [Avoiding common pitfalls when clustering biological data](https://stke.sciencemag.org/content/9/432/re6)

## Review Questions

1. What is clustering? What does it seek to accomplish?
2. What is agglomerative clustering? What does the resulting output look like?
3. What is the difference between single-link, complete-link, and average clustering?
4. What are three properties that are required of a distance metric? What kind of things can be compared with a distance metric?
5. What decisions does one have to make before performing k-means clustering?
6. Describe the process of fitting that k-means performs.
7. You and your colleague each run k-means with the same settings, and arrive at different results. How could this happen?
8. What are two cases in which k-means clustering can fail or provide bad results?
9. How do you determine that your clustering is "good"?
